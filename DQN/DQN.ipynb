{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for get n-uplet de theta\n",
    "from itertools import permutations\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    # time\n",
    "    T=1000\n",
    "    # factor discounted\n",
    "    beta=None\n",
    "    # number of actions(choose of allocation)\n",
    "    K = 3\n",
    "    # theta real for each action\n",
    "    theta_true = [0.65,0.40,0.25]\n",
    "    # theta discrete(possible, finite) estimated of each action\n",
    "    theta_possible = [0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1]\n",
    "    # NO pays a fee for each action if QoS less then threshold\n",
    "    gamma = 55\n",
    "    \n",
    "    # all possible theta joint(vector of theta) \n",
    "    # which respect the contraint of theta_1 > theta_2 > ... > theta_K\n",
    "    theta_joint=[]\n",
    "    # the belief\n",
    "    state = {}\n",
    "    # all possible actions\n",
    "    actions = None\n",
    "    \n",
    "    # input size for the Q-network\n",
    "    input_size = None\n",
    "    # the input of the first layer of network\n",
    "    x_input = None\n",
    "    # the input of target of network\n",
    "    target_input = None\n",
    "    # all value of loss of each iteration(training)\n",
    "    history_loss = None\n",
    "    # loss function\n",
    "    loss = None\n",
    "    # learning rate \n",
    "    learning_rate = None\n",
    "    # GradientDescentOptimizer of training\n",
    "    train_op = None\n",
    "    # the layer of prediction of network\n",
    "    prediction = None\n",
    "    \n",
    "    # number of iteration\n",
    "    iteration = None\n",
    "    # epsilon(probability) of epsilon-greedy \n",
    "    epsilon = None\n",
    "    \n",
    "    # session of network which manage the run of the network\n",
    "    session = None\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.0002, beta = 0.9):\n",
    "        # initialize the variables\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.iteration = 0\n",
    "        self.epsilon = 0.8\n",
    "        self.history_loss = []\n",
    "        \n",
    "        # get all theta joint by <permutations>\n",
    "        # which respect the contraint of theta_1 > theta_2 > ... > theta_K\n",
    "        for theta in permutations(self.theta_possible,self.K):\n",
    "            legal = True\n",
    "            K = self.K\n",
    "            for i in range(K-1):\n",
    "                # test if the vector of theta respect the constraint or not\n",
    "                if theta[i] <= theta[i+1]:\n",
    "                    legal = False\n",
    "                    break\n",
    "            if legal:\n",
    "                # if respect\n",
    "                self.theta_joint.append(theta)\n",
    "        \n",
    "        # initialize the belief by probibility joint uniform\n",
    "        for i in range(len(self.theta_joint)):\n",
    "            self.state[self.theta_joint[i]] = 1/(len(self.theta_joint))\n",
    "            \n",
    "        # initilize the actions <0,1,2,...,K-1>\n",
    "        self.actions = [i for i in range(K)]\n",
    "        # input size of network = number of theta joint + number of action\n",
    "        self.input_size = len(self.theta_joint)+ K\n",
    "        \n",
    "        # build the network\n",
    "        self.create_network()\n",
    "    \n",
    "    def create_network(self):\n",
    "        # the input layer of data\n",
    "        self.x_input = tf.placeholder(shape=[None,self.input_size], dtype=tf.float32)\n",
    "        # the input layer of targer\n",
    "        self.target_input = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        \n",
    "        # the first layer\n",
    "        # has 16 nodes\n",
    "        # use relu as active function\n",
    "        neural_network_layer_1 = 16\n",
    "        l1 = self.add_layer(self.x_input, self.input_size, neural_network_layer_1, activation_function=tf.nn.relu)\n",
    "        \n",
    "        # the second layer\n",
    "        # has 8 nodes\n",
    "        # use relu as active function\n",
    "        neural_network_layer_2 = 8\n",
    "        l2 = self.add_layer(l1, neural_network_layer_1, neural_network_layer_2, activation_function=tf.nn.relu)\n",
    "        \n",
    "        # the output layer \n",
    "        # the shape of output (None, 1)\n",
    "        self.prediction = self.add_layer(l2, neural_network_layer_2,1,activation_function=None)\n",
    "        \n",
    "        # define the loss function\n",
    "        # if input pieces of data, then take the mean\n",
    "        self.loss = tf.reduce_mean(tf.reduce_sum(tf.square(self.target_input-self.prediction),reduction_indices=[1]))\n",
    "        # us GradientDescentOptimizer and minimizer the loss\n",
    "        self.train_op = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        # initialize the weights randomly of the network \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(init)\n",
    "    \n",
    "    def get_action(self):\n",
    "        '''\n",
    "        get action based on current state and weight of network by epsilon-greedy\n",
    "        epsilon reduce by iteration increasing\n",
    "        \n",
    "        get action randomly with a probability of epsilon\n",
    "        otherwize get action which can minimizer Q calculated by current network\n",
    "        \n",
    "        return : action, Q correspond action calculated by network\n",
    "        '''\n",
    "        a = None\n",
    "        # get all Q by the current network for each possible action\n",
    "        Q = [self.predict(self.state, i)[0][0] for i in range(self.K)]\n",
    "        \n",
    "        '''a = np.argmin(Q)\n",
    "        '''\n",
    "        #self.epsilon = 0\n",
    "        if random.uniform(0,1)<self.epsilon:\n",
    "            # get action randomly\n",
    "            a = np.random.randint(0,self.K)\n",
    "        else :\n",
    "            # get action which can minimizer the Q\n",
    "            a = np.argmin(Q)\n",
    "        if self.iteration%20 == 19 and self.epsilon>0.1:\n",
    "            self.epsilon -= self.epsilon/20\n",
    "        \n",
    "        return a,Q[a]\n",
    "    \n",
    "    def get_action2(self):\n",
    "        '''\n",
    "        get action which can minimizer Q calculated by current network\n",
    "        \n",
    "        return : action, Q correspond action calculated by network\n",
    "        '''\n",
    "        estimated_theta = []\n",
    "        rts = []\n",
    "        for a in range(self.K):\n",
    "            prob_Y_y_a = 0.0\n",
    "            y_a = 1\n",
    "            for joint in self.theta_joint:\n",
    "                prob_Y_y_a += self.prob_bernouill(joint[a], y_a)*self.state[joint]\n",
    "            estimated_theta.append(prob_Y_y_a)\n",
    "        a = np.argmin(estimated_theta)\n",
    "        return a,self.predict(self.state, a)[0][0]\n",
    "    \n",
    "    def allocate(self, a):\n",
    "        '''\n",
    "        simulate action to client\n",
    "        calculate the reword and\n",
    "        the client return 1 with a probability of real theta_a, or 0\n",
    "        \n",
    "        a : action choosed\n",
    "        \n",
    "        problem : I dont sure that the rework should be expected or not\n",
    "        \n",
    "        return : rework immediate, 1 or 0\n",
    "        '''\n",
    "        # calculate probability of belief of theta for action a\n",
    "        prob_Y_y_a = 0.0\n",
    "        y_a = 1\n",
    "        for joint in self.theta_joint:\n",
    "            prob_Y_y_a += self.prob_bernouill(joint[a], y_a)*self.state[joint]\n",
    "            \n",
    "        # calculate reword immediate by cost and expected fee should pay\n",
    "        rt = self.cost(a) + self.gamma*prob_Y_y_a\n",
    "        y_a = None\n",
    "        \n",
    "        # get the real theta of action\n",
    "        theta = self.theta_true[a]\n",
    "        # simulate\n",
    "        if random.uniform(0,1) <= theta:\n",
    "            y_a = 1\n",
    "        else:\n",
    "            y_a = 0\n",
    "        #rt = self.cost(a) + self.gamma*y_a\n",
    "        return rt,y_a\n",
    "        \n",
    "    \n",
    "    def prob_bernouill(self, theta, y):\n",
    "        '''\n",
    "        probability of Bernouill\n",
    "        '''\n",
    "        return np.power(theta, y)*np.power((1-theta), (1-y))\n",
    "    \n",
    "    def get_next_state(self, a, y_a):\n",
    "        '''\n",
    "        calculate the new belief of theta for each action\n",
    "        \n",
    "        a : action performed\n",
    "        y_a : 1 or 0, the result return by client(simulation)\n",
    "        \n",
    "        return : the state(belief) of next time\n",
    "        '''\n",
    "        # calculate probability of belief of theta for action a\n",
    "        prob_Y_y_a = 0.0\n",
    "        next_state = {}\n",
    "        for joint in self.theta_joint:\n",
    "            prob_Y_y_a += self.prob_bernouill(joint[a], y_a)*self.state[joint]\n",
    "        \n",
    "        # calculate the new probability\n",
    "        for joint in self.theta_joint:\n",
    "            theta = joint[a]\n",
    "            p1 = self.prob_bernouill(theta, y_a)\n",
    "            next_state[joint] = p1*self.state[joint]/prob_Y_y_a\n",
    "            \n",
    "        return next_state\n",
    "    \n",
    "    def transition(self, next_state):\n",
    "        '''\n",
    "        update current state\n",
    "        '''\n",
    "        self.state = next_state\n",
    "    \n",
    "    def get_target(self, r, next_state):\n",
    "        '''\n",
    "        calculate the target by r + minimun Q of state_t+1\n",
    "        \n",
    "        r : current reword by simulating action\n",
    "        next_state : the state of t+1 \n",
    "        \n",
    "        return : target\n",
    "        '''\n",
    "        Q = [self.predict(next_state, i)[0][0] for i in range(self.K)]\n",
    "        return self.beta * np.min(Q) + r\n",
    "            \n",
    "        \n",
    "    def train(self,s,a,y):\n",
    "        '''\n",
    "        train the network\n",
    "        \n",
    "        s : current state\n",
    "        a : action performed\n",
    "        y : the target\n",
    "        \n",
    "        return : the loss of current train\n",
    "        '''\n",
    "        # iteration increases\n",
    "        self.iteration += 1\n",
    "        x = self.get_x(s,a)\n",
    "        _,loss = self.session.run([self.train_op, self.loss], feed_dict={self.x_input:x,self.target_input:y})\n",
    "        # save the loss of current train\n",
    "        self.history_loss.append(loss)\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def predict(self,s,a):\n",
    "        '''\n",
    "        get the output of network by input\n",
    "        \n",
    "        s : current stata\n",
    "        a : action performed\n",
    "        \n",
    "        return : Q predicted by network\n",
    "        '''\n",
    "        # calculate the data can be inputed by state and action\n",
    "        x = self.get_x(s,a)\n",
    "        return self.session.run(self.prediction, feed_dict={self.x_input:x})\n",
    "    \n",
    "    def evaluate(self):\n",
    "        '''\n",
    "        evaluate the network, and return:\n",
    "        1. estimated(expected) theta for each action\n",
    "        2. list of Q for each action of current state\n",
    "        3. expected reword of each action of current belief\n",
    "        '''\n",
    "        estimated_theta = []\n",
    "        rts = []\n",
    "        for a in range(self.K):\n",
    "            prob_Y_y_a = 0.0\n",
    "            y_a = 1\n",
    "            for joint in self.theta_joint:\n",
    "                prob_Y_y_a += self.prob_bernouill(joint[a], y_a)*self.state[joint]\n",
    "            estimated_theta.append(prob_Y_y_a)\n",
    "            rts.append(self.cost(a) + self.gamma*prob_Y_y_a)\n",
    "        print('  estimated thetas: ', estimated_theta)\n",
    "        print('  Q               : ', [self.predict(self.state, i)[0][0] for i in range(self.K)])\n",
    "        print('  rt              : ', rts)\n",
    "    \n",
    "    def get_x(self,s,a):\n",
    "        '''\n",
    "        get data can be input to the network\n",
    "        '''\n",
    "        return np.array([np.append(self.F(s), self.G(a))])\n",
    "        \n",
    "    def F(self,s):\n",
    "        '''\n",
    "        function maps state to vector\n",
    "        '''\n",
    "        return np.array([s[joint] for joint in self.theta_joint])\n",
    "    \n",
    "    def G(self,k):\n",
    "        '''\n",
    "        function maps state to vector\n",
    "        '''\n",
    "        r = np.zeros(self.K)\n",
    "        r[k]=1\n",
    "        return r\n",
    "    \n",
    "    def cost(self,k):\n",
    "        '''\n",
    "        cost of each action\n",
    "        '''\n",
    "        return 10*(k+1)\n",
    "    \n",
    "    def add_layer(self,inputs,in_size,out_size,activation_function=None):\n",
    "        '''\n",
    "        create one layer of network\n",
    "        \n",
    "        inputs : the input of the layer\n",
    "        in_size : input size\n",
    "        out_size : output size (number of nodes)\n",
    "        activation_function : activation function of the layer\n",
    "        '''\n",
    "        w = tf.Variable(tf.random_normal([in_size,out_size]))\n",
    "        b = tf.Variable(tf.zeros([1,out_size])+0.1)\n",
    "        f = tf.matmul(inputs,w) + b\n",
    "        if activation_function is None:\n",
    "            outputs = f\n",
    "        else:\n",
    "            outputs = activation_function(f)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new a DQN\n",
    "dqn = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show current state (belief)\n",
    "dqn.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "dqn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train the network\n",
    "\n",
    "# iteration\n",
    "t = 800\n",
    "for i in range(t):\n",
    "    # get action, and Q of the action\n",
    "    a_t, Q_t = dqn.get_action()\n",
    "    # allocate the choix to the client, and get the rework immediate and Y of a(1 of 0)\n",
    "    r_t, y_t = dqn.allocate(a_t)\n",
    "    # calculate the next state\n",
    "    next_state = dqn.get_next_state(a=a_t,y_a=y_t)\n",
    "    # calculate the target for training the network\n",
    "    target = dqn.get_target(r_t, next_state)\n",
    "    # print \n",
    "    print('at: %d  Qt: %.3f  rt: %.3f  yt: %d  target:%.3f'%(a_t,Q_t,r_t,y_t,target))\n",
    "    # train the network\n",
    "    dqn.train(dqn.state, a_t, [target])\n",
    "    # transfer to next state\n",
    "    dqn.transition(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "dqn.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show history loss\n",
    "\n",
    "plt.plot([i for i in range(dqn.iteration)], dqn.history_loss)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the belief\n",
    "dqn.state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
